{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to install networkx with pip\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import colorsys\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gym_members_exercise_tracking.csv')\n",
    "display(df)\n",
    "display(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution of calories burned\n",
    "sns.histplot(df['Calories_Burned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['Workout_Type'].unique())\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Workout_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelCalories(amount):\n",
    "    if 0 <= amount < 500:\n",
    "        return \"Small\"\n",
    "    elif 500 <= amount < 1200:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Calories'] = df['Calories_Burned'].apply(labelCalories)\n",
    "\n",
    "# standerdize the data\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(df.drop('Calories', axis=1))\n",
    "\n",
    "# scaled_features = scaler.transform(df.drop('Calories', axis=1))\n",
    "# scaled_features_df = pd.DataFrame(scaled_features, columns=df.columns[:-1])\n",
    "\n",
    "# scaled_features_df.drop(['Calories_Burned'], axis=1, inplace=True)\n",
    "\n",
    "# display(scaled_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "cols.remove('Calories')\n",
    "\n",
    "corr = df[cols].corr(min_periods=15) #  'Max_BPM',  'Experience_Level', \n",
    "sns.heatmap(corr)\n",
    "# create a sorted list of columns based on absolute value correlation with calories\n",
    "corr = corr['Calories_Burned'].apply(abs).sort_values(ascending=False)\n",
    "\n",
    "corr.drop('Calories_Burned', inplace=True)\n",
    "\n",
    "display(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into x and y\n",
    "df.drop('Calories_Burned', axis=1, inplace=True)\n",
    "X = df.drop(['Calories'], axis=1)\n",
    "y = df['Calories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train desiscion tree and visulize it\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def test_dTree(depth, features_num, tests, print_report=False):\n",
    "\n",
    "    total_predictions = []\n",
    "    total_y_test = []\n",
    "\n",
    "    for i in range(tests):\n",
    "\n",
    "        dtree = DecisionTreeClassifier(max_depth=depth)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        X_train = X_train[corr[:features_num].index]\n",
    "        X_test = X_test[X_train.columns]\n",
    "\n",
    "        dtree.fit(X_train, y_train)\n",
    "\n",
    "        predictions = dtree.predict(X_test)\n",
    "\n",
    "        total_predictions.extend(predictions)\n",
    "        total_y_test.extend(y_test)\n",
    "\n",
    "    report = classification_report(total_y_test, total_predictions, output_dict=True, zero_division=0)\n",
    "\n",
    "    if print_report:\n",
    "        print(classification_report(total_y_test, total_predictions))\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def plot_tree(dtree, features):\n",
    "    fig = plt.figure(figsize=(25,20))\n",
    "\n",
    "    _ = tree.plot_tree(dtree,\n",
    "                        feature_names=features,\n",
    "                        class_names=['Small', 'Medium', 'Large'],\n",
    "                        filled=True)\n",
    "\n",
    "    # fig.savefig(\"decision_tree.png\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = 1 # changed from 100 so its will run faster\n",
    "\n",
    "max_depth = 17\n",
    "max_features = len(X.columns)\n",
    "\n",
    "f1_scores = np.zeros((max_depth, max_features))\n",
    "\n",
    "for i in range(max_depth):\n",
    "    for j in range(max_features):\n",
    "        depth = i + 1\n",
    "        feature_num = j + 1\n",
    "\n",
    "        report = test_dTree(depth, feature_num, tests)\n",
    "\n",
    "        print(f\"Depth: {depth}, Features: {feature_num}: {report['weighted avg']['f1-score']}\")\n",
    "\n",
    "        f1_scores[i][j] = report['weighted avg']['f1-score']\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(f1_scores, fmt=\".1f\",)\n",
    "\n",
    "best_score = np.max(f1_scores)\n",
    "best_score_index = np.where(f1_scores == best_score)\n",
    "\n",
    "\n",
    "\n",
    "plt.gca().add_patch(plt.Rectangle((best_score_index[1][0], best_score_index[0][0]), 1, 1, fill=False, edgecolor='red', lw=3))\n",
    "\n",
    "\n",
    "plt.xlabel('Top N Features by correlation')\n",
    "plt.ylabel('Max Depth')\n",
    "plt.title('F1 Score for Decision Tree on test data')\n",
    "\n",
    "#save\n",
    "\n",
    "# plt.savefig(\"f1_score_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias the f1 scores withe a combination of teh niumber of features and the depth then plot again\n",
    "\n",
    "f1_scores_bias = np.zeros((max_depth, max_features))\n",
    "\n",
    "for i in range(max_depth):\n",
    "    for j in range(max_features):\n",
    "        depth = i + 1\n",
    "        feature_num = j + 1\n",
    "\n",
    "        f1_scores_bias[i][j] = f1_scores[i][j] - .15*((feature_num/max_features)*(depth/max_depth))\n",
    "\n",
    "sns.heatmap(f1_scores_bias, fmt=\".1f\",)\n",
    "\n",
    "best_score_b = np.max(f1_scores_bias)\n",
    "best_score_index_b = np.where(f1_scores_bias == best_score_b)\n",
    "\n",
    "\n",
    "\n",
    "plt.gca().add_patch(plt.Rectangle((best_score_index_b[1][0], best_score_index_b[0][0]), 1, 1, fill=False, edgecolor='red', lw=3))\n",
    "\n",
    "\n",
    "plt.xlabel('Top N Features sorted by correlation')\n",
    "plt.ylabel('Max Depth')\n",
    "plt.title('F1 Score with bias for Decision Tree on test data')\n",
    "\n",
    "#save\n",
    "\n",
    "# plt.savefig(\"f1_score_heatmap_bias.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best F1 Score: {best_score}, Depth: {best_score_index[0][0] + 1}, Features: {best_score_index[1][0] + 1}\")\n",
    "print(f\"Best F1 Score with bias: {best_score_b}, Depth: {best_score_index_b[0][0] + 1}, Features: {best_score_index_b[1][0] + 1}\")\n",
    "\n",
    "print(f\"unbiased f1 score: {f1_scores[best_score_index_b[0][0]][best_score_index_b[1][0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tree with best depth and features\n",
    "\n",
    "test_dTree(best_score_index_b[0][0] + 1, best_score_index_b[1][0] + 1, 100, True)\n",
    "\n",
    "dtree = DecisionTreeClassifier(max_depth=best_score_index_b[0][0] + 1)\n",
    "\n",
    "dtree.fit(X, y)\n",
    "\n",
    "plot_tree(dtree, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gym_members_exercise_tracking.csv')\n",
    "display(df)\n",
    "display(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelCalories(amount):\n",
    "    if 0 <= amount < 500:\n",
    "        return \"Small\"\n",
    "    elif 500 <= amount < 1200:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Calories'] = df['Calories_Burned'].apply(labelCalories)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age, Fat_Percentage, Height (m), 'Weight (kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[['Age', 'Gender', 'Weight (kg)', 'Height (m)','Avg_BPM',\n",
    "       'Resting_BPM', 'Session_Duration (hours)', 'Calories_Burned',\n",
    "       'Workout_Type', 'Fat_Percentage', 'Water_Intake (liters)',\n",
    "       'Workout_Frequency (days/week)','BMI', 'Calories']].corr(min_periods=15) #  'Max_BPM',  'Experience_Level', \n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age, Fat_Percentage, Height (m), 'Weight (kg)\n",
    "X = df.drop(['Gender','Avg_BPM','Resting_BPM', 'Session_Duration (hours)', 'Calories_Burned',\n",
    "             'Workout_Type', 'Water_Intake (liters)', 'Workout_Frequency (days/week)','BMI', 'Calories'],axis=1)\n",
    "y = df['Calories']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ann(mlp):\n",
    "    hidden_layers_n = len(mlp.coefs_)-1\n",
    "    layers_n = hidden_layers_n + 2\n",
    "    input_neurons_n = len(mlp.coefs_[0])\n",
    "    hidden_neurons_n = [len(mlp.coefs_[i+1]) for i in range(hidden_layers_n)]\n",
    "    output_neurons_n = len(mlp.coefs_[-1][0])\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    pos = {}\n",
    "\n",
    "    # Create the neurons of the input layer\n",
    "    for i in range(input_neurons_n):\n",
    "        pos['Layer0_{}'.format(i)] = (i,layers_n-1)\n",
    "\n",
    "    for j in range(hidden_layers_n):\n",
    "        # Create the neurons of the j'th hidden layer\n",
    "        prev_layer = j\n",
    "        cur_layer = j+1\n",
    "        if (j == 0):\n",
    "            prev_size = input_neurons_n\n",
    "        else:\n",
    "            prev_size = hidden_neurons_n[j-1]\n",
    "        for i in range(hidden_neurons_n[j]):\n",
    "            pos['Layer{}_{}'.format(cur_layer,i)] = (i,layers_n-1-cur_layer)\n",
    "            for k in range(prev_size):\n",
    "                w = mlp.coefs_[prev_layer][k][i]\n",
    "                G.add_edge('Layer{}_{}'.format(prev_layer,k),'Layer{}_{}'.format(cur_layer,i), weight=w)\n",
    "\n",
    "    # Create the neurons of the output layer\n",
    "    prev_layer = hidden_layers_n\n",
    "    cur_layer = hidden_layers_n+1\n",
    "    for i in range(output_neurons_n):\n",
    "        pos['Layer{}_{}'.format(cur_layer,i)] = (i,layers_n-1-cur_layer)\n",
    "        for k in range(hidden_neurons_n[-1]):\n",
    "            w = mlp.coefs_[prev_layer][k][i]\n",
    "            G.add_edge('Layer{}_{}'.format(prev_layer,k),'Layer{}_{}'.format(cur_layer,i), weight=w)\n",
    "\n",
    "    edges = G.edges()\n",
    "    colors = [colorsys.hsv_to_rgb(0 if G[u][v]['weight'] < 0 else 0.65,\n",
    "                                  1,#min(1, abs(G[u][v]['weight'])),\n",
    "                                  1) for u,v in edges]\n",
    "    weights = [abs(G[u][v]['weight'])*2 for u,v in edges]\n",
    "\n",
    "    nx.draw(G, pos, node_color='y', node_size=450, width=weights, edge_color=colors)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNetwork(hiddenLayer):\n",
    "    # mlp = MLPClassifier(hidden_layer_sizes=(5),max_iter=500)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hiddenLayer,max_iter=50000)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    predictions = mlp.predict(X_test)\n",
    "    print(confusion_matrix(y_test,predictions))\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print('This dataset has {} input nodes and {} output node(s)'.format(len(X.columns), len(y.unique())))\n",
    "    print('There are {} 2D arrays of coefficients, one for each layer'.format(len(mlp.coefs_)))\n",
    "    print('The layers have the following number of coefficients: {}')\n",
    "    for l in range(len(mlp.coefs_)):\n",
    "        m = len(mlp.coefs_[l])\n",
    "        n = len(mlp.coefs_[l][0])\n",
    "        print('  {}: {}x{} ({} nodes feeding into a layer of {} nodes)'.format(l, m, n, m, n))\n",
    "    # Print the actual coefficients\n",
    "    # print(mlp.coefs_)\n",
    "\n",
    "    print()\n",
    "    print('There are {} 1D arrays of intercepts, one for each layer'.format(len(mlp.intercepts_)))\n",
    "    print('Each layer has {} intercepts, one for each node'.format([len(mlp.intercepts_[l]) for l,_ in enumerate(mlp.intercepts_)]))\n",
    "    show_ann(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeNetwork((13, 13, 2, 14, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeNetwork((13, 13,14, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeNetwork((13, 14, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeNetwork((13, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeNetwork((17, 14))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
